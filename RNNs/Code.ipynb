{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSSL Lab 4 - Recurrent Neural Networks\n",
    "In this lab we will explore different RNN models and training procedures for a problem in time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)  # Increase default size of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed, for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and prepare the data\n",
    "We will build a model for predicting the number of [sunspots](https://en.wikipedia.org/wiki/Sunspot). We work with a data set that has been published on [Kaggle](https://www.kaggle.com/robervalt/sunspots), with the description:\n",
    "\n",
    "_Sunspots are temporary phenomena on the Sun's photosphere that appear as spots darker than the surrounding areas. They are regions of reduced surface temperature caused by concentrations of magnetic field flux that inhibit convection. Sunspots usually appear in pairs of opposite magnetic polarity. Their number varies according to the approximately 11-year solar cycle._\n",
    "\n",
    "The data consists of the monthly mean total sunspot number, from 1749-01-01 to 2017-08-31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data=pandas.read_csv('Sunspots.csv',header=0)\n",
    "dates = data['Date'].values\n",
    "y = data['Monthly Mean Total Sunspot Number'].values\n",
    "ndata=len(y)\n",
    "print(f'Total number of data points: {ndata}')\n",
    "\n",
    "# We define a train/test split, here with 70 % training data\n",
    "ntrain = int(ndata*0.7)\n",
    "ntest = ndata-ntrain\n",
    "print(f'Number of training data points: {ntrain}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates[:ntrain], y[:ntrain])\n",
    "plt.plot(dates[ntrain:], y[ntrain:])\n",
    "plt.xticks(range(0, ndata, 300), dates[::300], rotation = 90);  # Show only one tick every 25th year for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear seasonality to the data, but the amplitude of the peaks very quite a lot. Also, we note that the data is nonnegative, which is natural since it consists of counts of sunspots. However, for simplicity we will not take this constraint into account in this lab assignment and allow ourselves to model the data using a Gaussian likelihood (i.e. using MSE as a loss function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we see that the range of the data is roughly [0,400] so as a simple normalization we divide by the constant `MAX_VAL=400`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VAL = 400\n",
    "y = y/MAX_VAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline methods\n",
    "Before constructing any sophosticated models using RNNs, let's consider two baseline methods,\n",
    "\n",
    "1. The first baseline is a \"naive\" method which simply predicts $y_{t} = y_{t-1}$.\n",
    "2. The second baseline is an AR($p$) model (based on the implementation used for lab 1).\n",
    "\n",
    "We evaluate the performance of these method in terms of mean-squared-error and mean-absolute-error, to compare the more advanced models with later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalutate_performance(y_pred, y, split_time, name=None):\n",
    "    \"\"\"This function evaluates and prints the MSE and MAE of the prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : ndarrary\n",
    "        Array of size (n,) with predictions.\n",
    "    y : ndarray\n",
    "        Array of size (n,) with target values.\n",
    "    split_time : int\n",
    "        The leading number of elements in y_pred and y that belong to the training data set.\n",
    "        The remaining elements, i.e. y_pred[split_time:] and y[split_time:] are treated as test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute error in prediction\n",
    "    resid = y - y_pred\n",
    "    \n",
    "    # We evaluate the MSE and MAE in the original scale of the data, i.e. we add back MAX_VAL\n",
    "    train_mse = np.mean(resid[:split_time]**2)*MAX_VAL**2    \n",
    "    test_mse = np.mean(resid[split_time:]**2)*MAX_VAL**2\n",
    "    train_mae = np.mean(np.abs(resid[:split_time]))*MAX_VAL\n",
    "    test_mae = np.mean(np.abs(resid[split_time:]))*MAX_VAL\n",
    "    \n",
    "    # Print\n",
    "    print(f'Model {name}\\n  Training MSE: {train_mse:.4f},   MAE: {train_mae:.4f}\\n  Testing MSE:  {test_mse:.4f},   MAE: {test_mae:.4f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** Implement the naive baseline method which predicts according to $\\hat y_{t|t-1} = y_{t-1}$. Since the previous value is needed for the prediction we do not get a prediction at $t=1$. Hence, we evaluate the method by predicting values at $t=2, \\dots, n$ (cf. an AR($p$) model where we start predicting at $t=p+1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predictions in an array of length ndata-1. Note that there is a shift in the indices\n",
    "# between the prediction and the observation sequence, since there is no prediction available for the first observation.\n",
    "# Specifically, y_pred_naive[t] is a prediction of y[t+1], so the first element of y_pred_naive is a prediction of the\n",
    "# second element of y, and so on. We will use the same \"bookeeping convention\" throughout the lab, so it is important that\n",
    "# you understand it!\n",
    "y_pred_naive = ?????\n",
    "\n",
    "evalutate_performance(y_pred_naive,  # Predictions\n",
    "                      y[1:],         # Correspondsing target values\n",
    "                      ntrain-1,      # Number of leading elements in the input arrays corrsponding to training data points\n",
    "                      name='Naive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider a slightly more advanced baseline method, namely an AR($p$) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import two functions that were written as part of lab 1\n",
    "from tssltools_lab4 import fit_ar, predict_ar_1step\n",
    "\n",
    "p=30  # Order of the AR model (set by a few manual trials)\n",
    "ar_coef = fit_ar(y[:ntrain], p)  # Fit the model to the training data\n",
    "\n",
    "# Predict. Note that y contains both training and validation data,\n",
    "# and the prediction is for the values y_{p+1}, ..., y_{n}.\n",
    "y_pred_ar = predict_ar_1step(ar_coef, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalutate_performance(y_pred_ar,  # The prediction array is of length n-p\n",
    "                      y[p:],      # Corresponding target values\n",
    "                      ntrain-p,   # Number of leading elements in the input arrays corrsponding to training data points\n",
    "                      name='AR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple RNN\n",
    "We will now construct a model based on a recurrent neural network. We will initially use the `SimpleRNN` class from _Keras_, which correspond to the basic Jordan-Elman network presented in the lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Assume that we construct an \"RNN cell\" using the call `layers.SimpleRNN(units = d, return_sequences=True)`. Now, assume that an array `X` with the dimensions `[Q,M,P]` is fed as the input to the above object. We know that `X` contains a set of sequences (time series) with equal lengths. Specify which of the symbols ${Q,M,P}$ that corresponds to each of the items below:  \n",
    "- The length of the sequences (number of time steps)\n",
    "- The number of features (at each time step), i.e. the dimension of each time series\n",
    "- The number of sequences\n",
    "\n",
    "Furthermore, specify the values of ${Q,M,P}$ for the data at hand (treated as a single time series).\n",
    "\n",
    "_Hint:_ Read the documentation for [SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) to find the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Continuing the question above, answer the following:\n",
    "\n",
    "- What is the meaning of setting `units = d`?\n",
    "- Assume that we pass a single time series of length $n$ as input to the layer. Then what is the dimension of the _output_?\n",
    "- If we would had set the parameter `return_sequences=False` when constructing the layer, then what would be the answer to the previous question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _Keras_, each layer is created separately and are then joined by a `Sequential` object. It is very easy to construct stacked models in this way. The code below corresponds to a simple Jordan-Elman Network on the form,\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\mathbf{h}_{t} &= \\sigma( W\\mathbf{h}_{t-1} + U y_{t-1} + b ), \\\\\n",
    "\t\\hat y_{t|t-1} &= C \\mathbf{h_t} + c,\n",
    "\\end{align*}\n",
    "\n",
    "_Note:_ It is not necessary to explicitly specify the input shape, since this can be inferred from the input on the first call. However, for the `summary` function to work we need to tell the model what the dimension of the input is so that it can infer the correct sizes of the involved matrices. Also note that in _Keras_ you can sometimes use `None` when some dimensions are not known in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10  # hidden state dimension\n",
    "\n",
    "model0=keras.Sequential([\n",
    "    # Simple RNN layer\n",
    "    layers.SimpleRNN(units = d, input_shape=(None,1), return_sequences=True, activation='tanh'),\n",
    "    # A linear output layer\n",
    "    layers.Dense(units = 1, activation='linear')\n",
    "])\n",
    "    \n",
    "# We store the initial weights in order to get an exact copy of the model when trying different training procedures    \n",
    "model0.summary()\n",
    "init_weights = model0.get_weights().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** From the model summary we can see the number of paramters associated with each layer. Relate these numbers to the dimensions of the weight matrices and bias vectors $\\{W, U, b, C, c\\}$ in the mathematical model definition above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the RNN model\n",
    "In this section we will consider a few different ways of handling the data when training the simple RNN model constructed above. As a first step, however, we construct explicit input and target (output) arrays for the training and test data, which will simplify the calls to the training procedures below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task that we consider in this lab is one-step prediction, i.e. at each time step we compute a prediction $\\hat y_{t|t-1} \\approx y_t$ which depend on the previous observations $y_{1:t-1}$. However, when working with RNNs, the information contained in previous observations is aggregated in the _state_ of the RNN, and we will only use $y_{t-1}$ as the _explicit input_ at time step $t$.\n",
    "\n",
    "Furthermore, when addressing a problem of time series prediction it is often a good idea to introduce an explicit skip connection from the input $y_{t-1}$ to the prediction $\\hat y_{t|t-1}$. Equivalently, we can _define the target value_ at time step $t$ to be the residual $\\tilde y_t := y_t - y_{t-1}$. Indeed, if the model can predict the value of the residual, then we can simply add back $y_{t-1}$ to get a prediction of $y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking this into consideration, we define explicit input and output arrays as shifted versions of the data series $y_{1:n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "x_train = y[:ntrain-1]  # Input is denoted by x, training inputs are x[0]=y[0], ..., x[ntrain-1]=y[ntrain-1]\n",
    "yt_train = y[1:ntrain] - x_train  # Output is denoted by yt, training outputs are yt[0]=y[1]-y[0], ..., yt[ntrain-1] = y[ntrain]-y[ntrain-1]\n",
    "\n",
    "# Test data\n",
    "x_test = y[ntrain-1:-1]  # Test inputs are x_test[0] = y[ntrain-1], ..., x_test[ntest] = y[n-1]\n",
    "yt_test = y[ntrain:] - x_test  # Test outputs are yt_test[0] = y[ntrain]-y[ntrain-1], ..., yt_test[ntest] = y[n]-y[n-1] \n",
    "\n",
    "# Reshape the data\n",
    "x_train = x_train.reshape((1,ntrain-1,1))\n",
    "yt_train = yt_train.reshape((1,ntrain-1,1))\n",
    "x_test = x_test.reshape((1,ntest,1))\n",
    "yt_test = yt_test.reshape((1,ntest,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. Process all data in each gradient computation (\"do nothing\")\n",
    "The first option is to process all data at each iteration of the gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.clone_model(model0)  # This creates a new instance of the same model\n",
    "model1.set_weights(init_weights)  # We set the initial weights to be the same for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** What should we set the _batch size_ to, in order to compute the gradient based on the complete training data sequnce at each iteration? Complete the code below!\n",
    "\n",
    "_Note:_ You can set `verbose=1` if you want to monitor the training progress, but if you do, please **clear the output of the cell** before generating a pdf with your solutions, so that we don't get multiple pages with training errors in the submitted reports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])\n",
    "history = model1.fit(x_train, yt_train,\n",
    "                     epochs = 200,\n",
    "                     batch_size = ????,\n",
    "                     verbose = 0, \n",
    "                     validation_data = (x_test, yt_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the training and test error vs the iteration (epoch) number, using a helper function from the `tssltools_lab4` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tssltools_lab4 import plot_history\n",
    "start_at = 10  # Skip the first few epochs for clarity\n",
    "plot_history(history, start_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Finally we compute the predictions of $\\{y_t\\}$ for both the training and test data uning the model's `predict` function. Complete the code below to compute the predictions.\n",
    "\n",
    "_Hint:_ You need to reshape the data when passing it to the `predict` to comply with the input shape used in _Keras_ (cf. above).\n",
    "\n",
    "_Hint:_ Since the model is trained on the residuals $\\tilde y_t$, don't forget to add back $y_{t-1}$ when predicting $y_t$. However, make sure that you dont \"cheat\" by using a non-causal predictor (i.e. using $y_t$ when predicting $y_t$)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on all data using the final model.\n",
    "\n",
    "# We predict using y_1,...,y_{n-1} as inputs, resulting in predictions of the values y_2, ..., y_n.\n",
    "# That is, y_pred1 should be an (n-1,) array where element y_pred[t] is based only on values y[:t]\n",
    "y_pred1 = model1.predict( ????? ).flatten() + ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the prediction computed above we can plot them and evaluate the performance of the model in terms of MSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(y_pred):\n",
    "    # Plot prediction on test data\n",
    "    plt.plot(dates[ntrain:], y[ntrain:])\n",
    "    plt.plot(dates[ntrain:], y_pred[ntrain-1:])\n",
    "    plt.xticks(range(0, ntest, 300), dates[ntrain::300], rotation = 90);  # Show only one tick every 25th year for clarity\n",
    "    plt.legend(['Data','Prediction'])\n",
    "    plt.title('Predictions on test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction\n",
    "plot_prediction(y_pred1)\n",
    "    \n",
    "# Evaluate MSE and MAE (both training and test data)\n",
    "evalutate_performance(y_pred1, y[1:], ntrain-1, name='Simple RNN, \"do nothing\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Random windowing\n",
    "Instead of using all the training data when computing the gradient for the numerical optimizer, we can speed it up by restricting the gradient computation to a smaller window of consecutive time steps. Here, we sample a random window within the traing data and \"pretend\" that this window is independent from the observations outside the window. Specifically, when processing the observations within each window the hidden state of the RNN is initialized to zero at the first time point in the window.\n",
    "\n",
    "To implement this method in Python, we will make use of a _generator function_. A generator is a function that can be paused, return an intermediate value, and then resumed to continue its execution. An intermediate return value is produces using the `yield` keyword. \n",
    "\n",
    "Generators are used in _Keras_ to implement inifinite loops that feed the training procedure with training data. Specifically, the `yield` statement of the generator should return a pair `x, y` with inputs and corresponding targets from the training data. Each epoch of the training procedure will then call the generator for a total of `steps_per_epoch` such `yield` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train(window_size):    \n",
    "    while True:\n",
    "        \"\"\"The upper value is excluded in randint, so the maximum value that we can get is tt = ntrain-window_size-1.\n",
    "        Hence, the maximum end point of a window is ntrain-1, in agreement with the fact that the size of input/output is ntrain-1\n",
    "        when working with one-step-ahead prediction.\"\"\" \n",
    "        start_of_window = np.random.randint(0, ntrain - window_size)  # First time index of window (inclusive)\n",
    "        end_of_window = start_of_window + window_size  # Last time index of window (exclusive, i.e. this is really the first index _after_ the window)\n",
    "        yield x_train[:,start_of_window:end_of_window,:], yt_train[:,start_of_window:end_of_window,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.clone_model(model0)  # This creates a new instance of the same model\n",
    "model2.set_weights(init_weights)  # We set the initial weights to be the same for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Assume that we process a window of observations of length `window_size` at each iteration. Then, how many gradient steps per epoch can we afford, for computational cost per epoch to be comparable to the method considered in Option 1? Set the `steps_per_epoch` parameter of the fitting function based on your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "model2.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])\n",
    "history = model2.fit(generator_train(window_size),\n",
    "           epochs = 200,\n",
    "           verbose = 0, \n",
    "           steps_per_epoch = ????? , \n",
    "           validation_data = (x_test, yt_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to above we plot the error curves vs the iteration (epoch) number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history, start_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8:** Comparing this error plot to the one you got for training Option 1, can you see any _qualitative_ differences? Explain the reason for the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A8:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9:** Compute a prediction for all values of $\\{y_2, \\dots, y_n\\}$ analogously to **Q6**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict on all data using the final model.\n",
    "# We predict using y_1,...,y_{n-1} as inputs, resulting in predictions of the values y_2, ..., y_n\n",
    "y_pred2 = model2.predict( ????? ).flatten() + ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction on test data\n",
    "plot_prediction(y_pred2)\n",
    "    \n",
    "# Evaluate MSE and MAE (both training and test data)\n",
    "evalutate_performance(y_pred2, y[1:], ntrain-1, name='Simple RNN, windowing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3. Sequential windowing with stateful training\n",
    "As a final option we consider a model aimed at better respecting the temporal dependencies between consequtive windows. This is based on \"statefulness\" which simply means that the RNN remembers its hidden state between calls. That is, if model is in stateful mode and is used to process two sequences of inputs after each other, then the final state from the first sequence is used as the initial state for the second sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable stateful training, we need to create model where we set stateful=True in the RNN layer\n",
    "model3=keras.Sequential([\n",
    "    # Simple RNN layer with stateful=True\n",
    "    layers.SimpleRNN(units = d, batch_input_shape=(1,None,1), return_sequences=True, stateful=True, activation='tanh'),\n",
    "    # A linear output layer\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model3.set_weights(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10:** When working with stateful training we need to make some adjustments to the training data generator.\n",
    "\n",
    "1. First, the RNN model doesn't keep track of the actual time indices of the different windows that it is fed. Hence, if we feed the model randomly selected windows, it will still treat them as if they were consecutive, and retain the state from one window to the next. To avoid this, we therefore need to make sure that the generator outputs windows of training data that are indeed consecutive (and not ranomdly selected as above).\n",
    "\n",
    "2. When training the model we will process the whole training data multiple times (i.e. we train for multiple epochs). However, if we have statefulness _between epochs_ this would effectively result in a \"circular dependence\", where the final state at time step $t = n_{\\text{train}}$ would be used as the initial state at time $t=1$. To avoid this, we can manually reset the state of the model by calling `model.reset_states()`.\n",
    "\n",
    "Taking this two points into consideration, complete the code for the stateful data generator below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_stateful(window_size, model):\n",
    "    \"\"\"In addition to the window_size, the generator also takes the model as input so\n",
    "    that we can reset the RNN states at appropiate intervals.\"\"\"\n",
    "    \n",
    "    # Compute the total number of windows of length window_size that we need to cover all the training data.\n",
    "    #  Note 1. The length of x_train (and yt_train) is ntrain-1 since we work with 1-step prediction.\n",
    "    #  Note 2. The final window could be smaller than window_size, if (ntrain-1) is not evenly divisable by the window_size.    \n",
    "    number_of_windows = int( ????? )\n",
    "    \n",
    "    while True:\n",
    "        for i in range(number_of_windows):\n",
    "            # First time index of window (inclusive)\n",
    "            start_of_window = ?????\n",
    "            \n",
    "            # Last time index of window (exclusive, i.e. this is the index to the first time step after the window)\n",
    "            # Note 3. Python allows using end_of_window > ntrain-1, it will simply truncate the indexing at the final element of the array!\n",
    "            end_of_window = ?????\n",
    "            \n",
    "            yield x_train[:,start_of_window:end_of_window,:], yt_train[:,start_of_window:end_of_window,:]\n",
    "            \n",
    "    \"\"\"NOTE! In addition to replacing the ????? with the correct code, you need to move the line\"\"\"\n",
    "    model.reset_states()\n",
    "    \"\"\"to the correct place in the function definition above!\"\"\"\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the generator defined we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "model3.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])\n",
    "history = model3.fit(generator_train_stateful(window_size, model3),\n",
    "           epochs = 200,\n",
    "           verbose = 0, \n",
    "           steps_per_epoch = ntrain//window_size, \n",
    "           validation_data = (x_test, yt_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to above we plot the error curves vs the iteration (epoch) number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, start_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11:** Comparing this error plot to the one you got for training Options 1 and 2, can you see any _qualitative_ differences? \n",
    "\n",
    "_Optional:_ If you have a theory regarding the reason for the observed differences, feel free to explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A11:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12:** Compute a prediction for all values of $\\{y_2, \\dots, y_n\\}$ analogously to **Q6**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict on all data using the final model.\n",
    "# We predict using y_1,...,y_{n-1} as inputs, resulting in predictions of the values y_2, ..., y_n\n",
    "y_pred3 = model3.predict( ????? ).flatten() + ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction on test data\n",
    "plot_prediction(y_pred3)\n",
    "    \n",
    "# Evaluate MSE and MAE (both training and test data)\n",
    "evalutate_performance(y_pred3, y[1:], ntrain-1, name='Simple RNN, windowing/stateful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Reflection\n",
    "**Q13:** Which model performed best? Did you manage to improve the prediction compared to the two baseline methods? Did the RNN models live up to your expectations? Why/why not? Please reflect on the lab using a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A13:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A more complex network (OPTIONAL)\n",
    "If you are interested, feel free to play around with more complex models and see if you can improve the predictive performance! It is very easy to build stacked models in _Keras_, see the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A stacked model with 3 layers of LSTM cells, two Dense layers with Relu activation and a final linear output layer\n",
    "model4 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.LSTM(64, batch_input_shape=(1,None,1), return_sequences=True, stateful=True),\n",
    "  tf.keras.layers.LSTM(64, batch_input_shape=(1,None,1), return_sequences=True, stateful=True),\n",
    "  tf.keras.layers.LSTM(64, batch_input_shape=(1,None,1), return_sequences=True, stateful=True),\n",
    "  tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the best model in a file, so that we can load it after analyisng the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)  # Save only the best model, determined by the validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "model4.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])\n",
    "history = model4.fit(generator_train_stateful(window_size, model4),\n",
    "           epochs = 200,\n",
    "           verbose = 0, \n",
    "           steps_per_epoch = ntrain//window_size, \n",
    "           validation_data = (x_test, yt_test),\n",
    "           callbacks=[model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, start_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14 (optional):** Based on the training and test error plots, are there signs of over- or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A14:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the best model from checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on all data using the final model.\n",
    "# We predict using y_1,...,y_{n-1} as inputs, resulting in predictions of the values y_2, ..., y_n\n",
    "y_pred4 = model4.predict( ????? ).flatten() + ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on all data using the final model.\n",
    "# We predict using y_1,...,y_{n-1} as inputs, resulting in predictions of the values y_2, ..., y_n\n",
    "y_pred4 = model4.predict(y[:-1].reshape(1, ndata-1, 1)).flatten() + y[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction on test data\n",
    "plot_prediction(y_pred4)\n",
    "    \n",
    "# Evaluate MSE and MAE (both training and test data)\n",
    "evalutate_performance(y_pred4, y[1:], ntrain-1, name='Stacked RNN, windowing/stateful')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
